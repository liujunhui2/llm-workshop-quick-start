{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90e451ce",
   "metadata": {},
   "source": [
    "# 部署 Vicuna-13B模型\n",
    "\n",
    "本实验基于 HuggingFace Text Generate Inference （TGI）容器将 Vicuna-13B 模型部署到 Amazon SageMaker Endpoint。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f46b80",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install -U sagemaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238b182c",
   "metadata": {},
   "source": [
    "### 环境初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c480c92b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker role arn: arn:aws:iam::169088282855:role/AmazonSageMaker-ExecutionRole-20191205T170039\n",
      "sagemaker session region: us-west-2\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55187a4",
   "metadata": {},
   "source": [
    "### 获取专用于 SageMaker 的 tgi 镜像地址"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b745633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llm image uri: 763104351884.dkr.ecr.us-west-2.amazonaws.com/huggingface-pytorch-tgi-inference:2.0.0-tgi0.8.2-gpu-py39-cu118-ubuntu20.04\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.huggingface import get_huggingface_llm_image_uri\n",
    "\n",
    "# retrieve the llm image uri\n",
    "llm_image = get_huggingface_llm_image_uri(\n",
    "  \"huggingface\",\n",
    "  version=\"0.8.2\"\n",
    ")\n",
    "\n",
    "# print ecr image uri\n",
    "print(f\"llm image uri: {llm_image}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa66677e",
   "metadata": {},
   "source": [
    "### 设定模型相关配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "345914a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "\n",
    "# LLM config\n",
    "model_id = \"lmsys/vicuna-13b-v1.3\"\n",
    "instance_type = \"ml.g5.12xlarge\"\n",
    "number_of_gpu = 4\n",
    "health_check_timeout = 600\n",
    "\n",
    "# create HuggingFaceModel\n",
    "llm_model = HuggingFaceModel(\n",
    "  role=role,\n",
    "  image_uri=llm_image,\n",
    "  env={\n",
    "    'HF_MODEL_ID': model_id,\n",
    "    'SM_NUM_GPUS': json.dumps(number_of_gpu),\n",
    "    # 'HF_MODEL_QUANTIZE': \"bitsandbytes\", # comment in to quantize\n",
    "  }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53463e4e",
   "metadata": {},
   "source": [
    "开始部署！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "283806b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------!"
     ]
    }
   ],
   "source": [
    "# Deploy model to an endpoint\n",
    "# https://sagemaker.readthedocs.io/en/stable/api/inference/model.html#sagemaker.model.Model.deploy\n",
    "llm = llm_model.deploy(\n",
    "  initial_instance_count=1,\n",
    "  instance_type=instance_type,\n",
    "  # volume_size=400, # If using an instance with local SSD storage, volume_size must be None, e.g. p4 but not p3\n",
    "  container_startup_health_check_timeout=health_check_timeout, # 10 minutes to be able to load the model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed28151",
   "metadata": {},
   "source": [
    "### 开始推理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1326b664",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Here are five tips that may help you run faster:\\n\\n1. Incorporate regular stretching and strengthening exercises into your training routine to improve flexibility and muscular strength.\\n2. Make sure you have proper running form, including a slight forward lean, a high cadence (steps per minute), and a short stride length.\\n3. Run at a comfortable pace that allows you to maintain good form and rhythm. As you get fitter, you can gradually increase the intensity of your runs.\\n4. Incorporate interval training into your routine, where you alternate between periods of high-intensity running and periods of rest or low-intensity running.\\n5. Make sure you get enough rest and recovery time between runs. This will allow your body to repair and rebuild itself, which will ultimately help you run faster over time.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define payload\n",
    "prompt = \"\"\"You are an helpful Assistant.\n",
    "\n",
    "User: Give me 5 tips about how to run faster.\n",
    "Assistant:\"\"\"\n",
    "\n",
    "# hyperparameters for llm\n",
    "payload = {\n",
    "  \"inputs\": prompt,\n",
    "  \"parameters\": {\n",
    "    \"do_sample\": True,\n",
    "    \"top_p\": 0.9,\n",
    "    \"temperature\": 0.8,\n",
    "    \"max_new_tokens\": 512,\n",
    "    \"repetition_penalty\": 1.03,\n",
    "    \"stop\": [\"\\nUser:\",\"<|endoftext|>\",\"</s>\"]\n",
    "  }\n",
    "}\n",
    "\n",
    "# send request to endpoint\n",
    "response = llm.predict(payload)\n",
    "\n",
    "# print assistant respond\n",
    "assistant = response[0][\"generated_text\"][len(prompt):]\n",
    "assistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6748c0ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are several ways to start learning machine learning, here are a few recommendations:\n",
      "\n",
      "1. Start with the basics: Learn the fundamental concepts of machine learning such as supervised and unsupervised learning, regression, classification, clustering, and neural networks.\n",
      "2. Choose a programming language: Python is a popular choice for machine learning due to its simplicity and versatility. You can use libraries like NumPy, Pandas, Scikit-learn, TensorFlow, and PyTorch to implement machine learning algorithms.\n",
      "3. Practice with real-world datasets: Use publicly available datasets to practice your machine learning skills and gain hands-on experience.\n",
      "4. Read books and online tutorials: There are many excellent books and online tutorials available that can provide a comprehensive introduction to machine learning.\n",
      "5. Join a community or take a course: Joining a machine learning community or taking a course can provide you with additional support, resources, and guidance as you learn.\n",
      "\n",
      "Remember, machine learning is a complex field and it takes time and effort to master it. Be patient and persistent in your learning and don't be afraid to ask for help if you need it.\n",
      "CPU times: user 668 µs, sys: 2.68 ms, total: 3.35 ms\n",
      "Wall time: 5.39 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "new_prompt = f\"\"\"{prompt}{assistant}\n",
    "User: How would you recommend start learning Machine Learning?\n",
    "Assistant :\"\"\"\n",
    "# update payload\n",
    "payload[\"inputs\"] = new_prompt\n",
    "\n",
    "# send request to endpoint\n",
    "response = llm.predict(payload)\n",
    "\n",
    "# print assistant respond\n",
    "new_assistant = response[0][\"generated_text\"][len(new_prompt):]\n",
    "print(new_assistant)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7b3780",
   "metadata": {},
   "source": [
    "### 清空资源"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "76e29519",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.delete_model()\n",
    "llm.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
